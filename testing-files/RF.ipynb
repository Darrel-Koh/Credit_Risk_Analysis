{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data \n",
    "\n",
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the null value in each attributes\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_rev_hi_lim \n",
    "\n",
    "# Fill missing values in total_rev_hi_lim with Simple Imputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "total_rev_hi_lim = df['total_rev_hi_lim'].values.reshape(-1,1)\n",
    "total_rev_hi_lim_imputed = imputer.fit_transform(total_rev_hi_lim)\n",
    "df['total_rev_hi_lim'] = total_rev_hi_lim_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# home_ownership \n",
    "\n",
    "# Remove rows with value ANY\n",
    "\n",
    "df = df[df['home_ownership'] != 'ANY']\n",
    "df['home_ownership'].unique()\n",
    "\n",
    "# Level encoding for home ownership \n",
    "\n",
    "home_type = ['RENT', 'OWN', 'MORTGAGE', 'OTHER', 'NONE']  # Unique values for encoding\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the grades\n",
    "encoder.fit(home_type)\n",
    "\n",
    "# Encode the 'grade' column in the DataFrame\n",
    "df['home_ownership'] = encoder.transform(df['home_ownership'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose \n",
    "\n",
    "# Label encoding for purpose\n",
    "\n",
    "df['purpose'].unique()\n",
    "\n",
    "purposes = ['credit_card', 'car', 'small_business', 'other', 'wedding',\n",
    "       'debt_consolidation', 'home_improvement', 'major_purchase',\n",
    "       'medical', 'moving', 'vacation', 'house', 'renewable_energy',\n",
    "       'educational']\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the grades\n",
    "encoder.fit(purposes)\n",
    "# Encode the 'grade' column in the DataFrame\n",
    "df['purpose'] = encoder.transform(df['purpose'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_grade\n",
    "\n",
    "# Sort the order of subgrades and do label encoding\n",
    "\n",
    "subgrades = ['B2', 'C4', 'C5', 'C1', 'B5', 'A4', 'E1', 'F2', 'C3', 'B1', 'D1',\n",
    "       'A1', 'B3', 'B4', 'C2', 'D2', 'A3', 'A5', 'D5', 'A2', 'E4', 'D3',\n",
    "       'D4', 'F3', 'E3', 'F4', 'F1', 'E5', 'G4', 'E2', 'G3', 'G2', 'G1',\n",
    "       'F5', 'G5']\n",
    "\n",
    "def custom_sort_key(subgrade):\n",
    "    match = re.match(r'([A-Za-z]+)(\\d+)', subgrade)\n",
    "    letter = match.group(1)\n",
    "    number = int(match.group(2))\n",
    "    \n",
    "    return letter, number\n",
    "\n",
    "sorted_subgrades = sorted(subgrades, key=custom_sort_key)\n",
    "\n",
    "# Level encoding for sorted sub-grade \n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the grades\n",
    "encoder.fit(sorted_subgrades)\n",
    "\n",
    "# Encode the 'grade' column in the DataFrame\n",
    "df['sub_grade'] = encoder.transform(df['sub_grade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# employment_lengths\n",
    "\n",
    "employment_lengths = ['< 1 year', '1 year', '2 years', '3 years', '4 years', '5 years', '6 years', '7 years', '8 years', '9 years', '10+ years', 'nan']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder with unique values\n",
    "label_encoder.fit(employment_lengths)\n",
    "\n",
    "# Encode the attribute values\n",
    "df['emp_length'] = label_encoder.transform(df['emp_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mths_since_last_delinq\n",
    "\n",
    "df['mths_since_last_delinq'] = df['mths_since_last_delinq'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mths_since_last_record\n",
    "\n",
    "df['mths_since_last_record'] = df['mths_since_last_record'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revol_util\n",
    "# Handle missing value with imputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "revol_util = df['revol_util'].values.reshape(-1,1)\n",
    "\n",
    "revol_util_imputed = imputer.fit_transform(revol_util)\n",
    "\n",
    "df['revol_util'] = revol_util_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate annual_inc and annual_inc_joint\n",
    "\n",
    "df.loc[df['application_type'] == 'JOINT', 'annual_inc'] = df.loc[df['application_type'] == 'JOINT', 'annual_inc_joint']\n",
    "df = df.drop('annual_inc_joint', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dti_joint\n",
    "\n",
    "df.loc[df['application_type'] == 'JOINT', 'dti'] = df.loc[df['application_type'] == 'JOINT', 'dti_joint']\n",
    "df = df.drop('dti_joint', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verification_status_joint\n",
    "\n",
    "df.loc[df['application_type'] == 'JOINT', 'verification_status'] = df.loc[df['application_type'] == 'JOINT', 'verification_status_joint']\n",
    "df = df.drop('verification_status_joint', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term \n",
    "# Label encoding for term\n",
    "\n",
    "term = [' 36 months', ' 60 months']  # Unique values for encoding\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the grades\n",
    "encoder.fit(term)\n",
    "\n",
    "# Encode the 'grade' column in the DataFrame\n",
    "df['term'] = encoder.transform(df['term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verification_status\n",
    "\n",
    "veri = ['Not Verified', 'Source Verified', 'Verified']  # Unique values for encoding\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(veri)\n",
    "\n",
    "\n",
    "df['verification_status'] = encoder.transform(df['verification_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pymnt_plan\n",
    "\n",
    "plan = ['n', 'y']  # Unique values for encoding\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(plan)\n",
    "df['pymnt_plan'] = encoder.transform(df['pymnt_plan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# application_type\n",
    "\n",
    "type = ['INDIVIDUAL', 'JOINT'] # Unique values for encoding\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(type)\n",
    "\n",
    "df['application_type'] = encoder.transform(df['application_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initial_list_status\n",
    "\n",
    "status = ['f', 'w'] # Unique values for encoding\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(status)\n",
    "\n",
    "df['initial_list_status'] = encoder.transform(df['initial_list_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing last credit pull\n",
    "df['last_credit_pull_d'].fillna(\"25-07-2023\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit History Length:\n",
    "# Calculated as: last_credit_pull - earliest_cr_line\n",
    "def date_difference(date_str1, date_str2):\n",
    "    # Convert date strings to datetime objects\n",
    "    date_format = \"%d-%m-%Y\"\n",
    "    date1 = datetime.strptime(date_str1, date_format)\n",
    "    date2 = datetime.strptime(date_str2, date_format)\n",
    "\n",
    "    # Calculate the difference\n",
    "    difference = date2 - date1\n",
    "\n",
    "    # Return the difference in days\n",
    "    return difference.days\n",
    "\n",
    "df['credit_history_length'] = df.apply(lambda row: date_difference(row['earliest_cr_line'], row['last_credit_pull_d']), axis=1)\n",
    "\n",
    "# Swap the values and column names\n",
    "df['default_ind'], df['credit_history_length'] = df['credit_history_length'], df['default_ind']\n",
    "df.rename(columns={'default_ind': 'credit_history_length', 'credit_history_length': 'default_ind'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop not usable attributes \n",
    "\n",
    "remove_col = [\n",
    "    'id',\n",
    "    'member_id',\n",
    "    'emp_title',\n",
    "    'issue_d',\n",
    "    'desc',\n",
    "    'title',\n",
    "    'zip_code',\n",
    "    'addr_state',\n",
    "    'earliest_cr_line',\n",
    "    'last_pymnt_d',\n",
    "    'last_pymnt_amnt',\n",
    "    'next_pymnt_d',\n",
    "    'last_credit_pull_d',\n",
    "    'collections_12_mths_ex_med',\n",
    "    'mths_since_last_major_derog',\n",
    "    'policy_code',\n",
    "    'tot_coll_amt',\n",
    "    'tot_cur_bal', \n",
    "    'open_acc_6m',\n",
    "    'open_il_6m', \n",
    "    'open_il_12m', \n",
    "    'open_il_24m', \n",
    "    'mths_since_rcnt_il', \n",
    "    'total_bal_il', \n",
    "    'il_util', \n",
    "    'open_rv_12m' ,\n",
    "    'open_rv_24m', \n",
    "    'max_bal_bc', \n",
    "    'all_util', \n",
    "    'inq_fi', \n",
    "    'total_cu_tl', \n",
    "    'inq_last_12m',\n",
    "    'grade'\n",
    "]\n",
    "\n",
    "df = df.drop(remove_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize default_ind attribute\n",
    "\n",
    "sns.countplot(data=df, x='default_ind')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New feature using user defined transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CreditUtilizationRatioTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, include=True):\n",
    "        self.revol_bal_col = 'revol_bal'\n",
    "        self.annual_inc_col = 'annual_inc'\n",
    "        self.installment_col = 'installment'\n",
    "        self.total_rec_prncp_col = 'total_rec_prncp'\n",
    "        self.funded_amnt_col = 'funded_amnt'\n",
    "\n",
    "        self.include_rev_to_inc_ratio = include\n",
    "        self.include_loan_to_inc_ratio = include\n",
    "        self.include_repayment_progress = include\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Find index of columns\n",
    "        revol_bal_ix = np.where(X.columns == self.revol_bal_col)[0][0]\n",
    "        annual_inc_ix = np.where(X.columns == self.annual_inc_col)[0][0]\n",
    "        installment_ix = np.where(X.columns == self.installment_col)[0][0]\n",
    "        total_rec_prncp_ix = np.where(X.columns == self.total_rec_prncp_col)[0][0]\n",
    "        funded_amnt_ix = np.where(X.columns == self.funded_amnt_col)[0][0]\n",
    "\n",
    "        # Calculate the Revolving Credit Balance to Annual Income Ratio.\n",
    "        rev_to_inc_ratio = X.iloc[:, revol_bal_ix] / X.iloc[:, annual_inc_ix]\n",
    "\n",
    "        # Calculate the Loan Payment-to-Income Ratio.\n",
    "        loan_to_inc_ratio = X.iloc[:, installment_ix] / (X.iloc[:, annual_inc_ix] / 12)\n",
    "\n",
    "        # Calculate the Repayment Progress.\n",
    "        repayment_progress = (X.iloc[:, total_rec_prncp_ix] / X.iloc[:, funded_amnt_ix]) * 100\n",
    "\n",
    "        if self.include_rev_to_inc_ratio:\n",
    "            # Add the calculated Revolving Credit Balance to Annual Income Ratio as a new column to the input data.\n",
    "            X['Rev_to_Inc_Ratio'] = rev_to_inc_ratio\n",
    "\n",
    "        if self.include_loan_to_inc_ratio:\n",
    "            # Add the calculated Loan Payment-to-Income Ratio as a new column to the input data.\n",
    "            X['Loan_Payment_to_Income_Ratio'] = loan_to_inc_ratio\n",
    "\n",
    "        if self.include_repayment_progress:\n",
    "            # Add the calculated Repayment Progress as a new column to the input data.\n",
    "            X['Repayment_Progress'] = repayment_progress\n",
    "\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize - Correlation matrix\n",
    "\n",
    "# Create a correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Select the correlation values with 'default_ind'\n",
    "target_corr = corr_matrix['default_ind']\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Fresh round of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a: Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    'collection_recovery_fee'   ,                  \n",
    "    'acc_now_delinq' ,                                    \n",
    "    'funded_amnt'   ,               \n",
    "    'funded_amnt_inv'   ,            \n",
    "    'mths_since_last_record' ,       \n",
    "    'delinq_2yrs'   ,                          \n",
    "    'dti'            ,              \n",
    "    'mths_since_last_delinq'  ,       \n",
    "    'emp_length',                      \n",
    "    'pub_rec'   ,                  \n",
    "    'revol_bal'    ,                                \n",
    "    'credit_history_length'  ,        \n",
    "    'term'   ,                       \n",
    "    'home_ownership' ,                                \n",
    "    'total_rev_hi_lim'  ,              \n",
    "    'total_pymnt'      ,                     \n",
    "    'total_pymnt_inv'  ,              \n",
    "    'purpose'   ,                   \n",
    "    'revol_util'   ,                 \n",
    "    'total_rec_int'   ,               \n",
    "    'inq_last_6mths'  ,              \n",
    "    'total_rec_prncp' ,                        \n",
    "    'sub_grade'        ,             \n",
    "    'total_rec_late_fee'   ,          \n",
    "    'int_rate'   ,                    \n",
    "    'out_prncp_inv'  ,                 \n",
    "    'out_prncp'   ,                       \n",
    "    'recoveries'                     \n",
    "]\n",
    "X = df[selected_features]\n",
    "y = df['default_ind']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b: Oversampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def Oversampling(X, y, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=random_state)\n",
    "    # Instantiate the SMOTE object\n",
    "    smote = SMOTE(random_state=42)\n",
    "    # Perform SMOTE only on the training data\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "    return X_train_balanced, X_test, y_train_balanced, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c: RF classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_X_train_balanced, RF_X_test, RF_y_train_balanced, RF_y_test = Oversampling(X, y)\n",
    "\n",
    "# will take 1 min to train\n",
    "rf_classifier = RandomForestClassifier(random_state=42, n_estimators=10)\n",
    "rf_classifier.fit(RF_X_train_balanced, RF_y_train_balanced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_y_pred = rf_classifier.predict(RF_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b: StratifiedShuffleSplit Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_X2_train, RF_X2_test, RF_y2_train, RF_y2_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "rf_classifier_default = RandomForestClassifier(random_state=42, n_estimators=10)\n",
    "rf_classifier_default.fit(RF_X2_train, RF_y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_y_pred_default = rf_classifier_default.predict(RF_X2_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Fine tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a: + User-defined Transformer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide to use new generated features\n",
    "\n",
    "RF_transformer_with_features = CreditUtilizationRatioTransformer(include=True)\n",
    "\n",
    "# Apply the transformer to add the new features to df.\n",
    "RF_df = RF_transformer_with_features.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_selected_features = selected_features \n",
    "\n",
    "RF_selected_features.append('Repayment_Progress')\n",
    "RF_selected_features.append('Loan_Payment_to_Income_Ratio')\n",
    "RF_selected_features.append('Rev_to_Inc_Ratio')\n",
    "RF_X1 = RF_df[RF_selected_features]\n",
    "RF_y1 = RF_df['default_ind']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b: Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_X1_train_balanced, RF_X1_test, RF_y1_train_balanced, RF_y1_test = Oversampling(RF_X1, RF_y1)\n",
    "\n",
    "# around 4 mins to train\n",
    "rf_classifier_tuned = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "RF_param_grid = {\n",
    "    'max_depth': [5, 6, 7, 8, 9],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "RF_grid_search = GridSearchCV(rf_classifier_tuned, RF_param_grid, cv=5, scoring='f1', verbose=1, n_jobs=-1)\n",
    "RF_best_model = RF_grid_search.fit(RF_X1_train_balanced, RF_y1_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_y_pred_tuned = RF_best_model.predict(RF_X1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Evaluate the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a: Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(y_true, y_pred, model_name):\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Display the confusion matrix as a heatmap\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Non-default', 'Default'], yticklabels=['Non-default', 'Default'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(model_name)\n",
    "    plt.show()\n",
    "\n",
    "    # Generate the classification report\n",
    "    report = classification_report(y_true, y_pred, target_names=['Non-default', 'Default'], output_dict=True)\n",
    "\n",
    "    # Create a summary table\n",
    "    summary_table = pd.DataFrame({\n",
    "        'Model': [model_name],\n",
    "        'Accuracy': [accuracy],\n",
    "        'ROC-AUC Score': [roc_auc],\n",
    "        'Precision (Non-default)': [report['Non-default']['precision']],\n",
    "        'Recall (Non-default)': [report['Non-default']['recall']],\n",
    "        'F1-score (Non-default)': [report['Non-default']['f1-score']],\n",
    "        'Precision (Default)': [report['Default']['precision']],\n",
    "        'Recall (Default)': [report['Default']['recall']],\n",
    "        'F1-score (Default)': [report['Default']['f1-score']],\n",
    "    })\n",
    "\n",
    "    return summary_table\n",
    "\n",
    "# Usage:\n",
    "basic_summary = evaluate_predictions(RF_y_test, RF_y_pred_default, \"Basic Model\")\n",
    "balanced_summary = evaluate_predictions(RF_y_test, RF_y_pred, \"Balanced Model\")\n",
    "tuned_summary = evaluate_predictions(RF_y_test, RF_y_pred_tuned, \"Fine-Tuned Model\")\n",
    "\n",
    "# Combine all summaries into one DataFrame\n",
    "all_summaries = pd.concat([basic_summary, balanced_summary, tuned_summary], ignore_index=True)\n",
    "print(all_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the result, we can see that even with fine - tuning, the accuracy of random forest model is high, around 99.7%. However, the model before fine tuning still miss out a lot of default values. For this case, since default is not good, we are trying to predict as much true default as possible. <br> \n",
    "\n",
    "After the fine tuning, we can see that the accuracy slightly increased, and the true default that is detected is also increased, and it only missed out 225 default cases. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
